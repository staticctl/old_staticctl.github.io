### 贝叶斯定理

在概率论与统计学中，贝叶斯定理 ($Bayes' theorem$) 或称为贝叶斯法则 ($Bayes' law or Bayes' rule$) 表达了一个事件发生的概率，而确定这一概率的方法是基于与该事件相关的条件先验知识 ($prior knowledge$)。例如，如果患癌症是与人的年龄相关的，那么使用贝叶斯方法，我们可以利用患癌症人群的年龄分布这个先验知识评判一个人患癌症的概率，这相比于不利用年龄信息去判断一个人是否患癌症会聪明得多。可见，上述过程是贝叶斯定理的一种实际应用，通常我们称之为：**贝叶斯推断 ($Bayesian inference$)**。

1.  条件概率

   条件概率，就是在一个事件发生的情况下，去判断另一个相关联的事件发生的概率，或者简单说，就是指在事件 B 发生的情况下，事件 A 发生的概率。通常记为$P(A|B)$。

   易知：
   $$
   P(A \mid B)=\frac{P(A \cap B)}{P(B)}
   $$

   $$
   P(A \cap B)=P(A \mid B) P(B)
   $$

   $$
   P(A \cap B)=P(B \mid A) P(A)
   $$

   可以得到
   $$
   P(A \mid B) P(B)=P(B \mid A) P(A)
   $$
   所以贝叶斯公式
   $$
   P(A \mid B)=\frac{P(B \mid A) P(A)}{P(B)}
   $$

2. 朴素贝叶斯分类

   类似于机器学习，输入特征向量$\mathcal{X}$，输出的类别标记为（class label）$\mathcal{Y}$，$XY$分别为输入空间$\mathcal{X}, \mathcal{Y}$上的随机变量，$P(X, Y)$是X和Y的联合概率分布。训练数据集为：
   $$
   T=\left(x_1, y_1\right),\left(x_2, y_2\right), \cdots,\left(x_n, y_n\right)
   $$
   由$P(X, Y)$独立同分布产生

   朴素贝叶斯方法通过训练数据集学习这个联合概率分布$P(X, Y)$，具体而言，它需要学习两个部分，第一是先验概率分布：
   $$
   P\left(Y=c_k\right), k=1,2, \cdots, K
   $$
   第二是条件概率分布：
   $$
   P\left(X=x \mid Y=c_k\right)=P\left(X^{(1)}=x_1, \ldots, X^{(d)}=x^{(d)} \mid Y=c_k\right), k=1,2, \cdots, K
   $$
   (注意：这里 X 的上标 (d) 是表示输入的维度，总共有 d 维)。

   **朴素贝叶斯法 (naive Bayes) 采用了 “属性条件独立性假设”**，对已知的类别，假设所有属性相互独立，换言之，假设每个属性独立地对分类结果发生影响。由此，条件概率可以重写为：
   $$
   P(x \mid c)=\prod_{i=1}^d P\left(x_i \mid c\right)
   $$
   再回到贝叶斯定理，我们的目标是给定一个 x ，推断其后验概率分布 P(c∣x) ，即该条数据属于每个类的概率是多少，然后选择概率最大的类别作为 x 的类输出。那么将贝叶斯定理结合属性条件独立性假设，可以得到：
   $$
   P(c \mid x)=\frac{P(x \mid c) P(c)}{P(x)}=\frac{P(c)}{P(x)} \prod_{i=1}^d P\left(x_i \mid c\right)
   $$
   由于对所有类别来说 P(x) 相同，因此贝叶斯判定准则可写为：
   $$
   \hat{y}=\arg \max _{c \in Y} P(c) \prod_{i=1}^d P\left(x_i \mid c\right)
   $$
   这就是朴素贝叶斯分类器

3. **朴素贝叶斯参数估计**

   实际在机器学习的分类问题的应用中，朴素贝叶斯分类器的训练过程就是基于训练集 $D$ 来估计类先验概率 $P(c)$ ，并为每个属性估计条件概率 $P(x_i∣c) $。这里就需要使用极大似然估计 (maximum likelihood estimation, 简称 $MLE$) 来估计相应的概率。

   令 $D_c$ 表示训练集$ D $中的第$ c $类样本组成的集合，若有充足的独立同分布样本，则可容易地估计出类别的先验概率：
   $$
   P(c)=\frac{\left|D_c\right|}{|D|}
   $$
   对于离散属性而言，令 $Dc,xi$ 表示$ Dc$ 中在第$ i$ 个属性上取值为$ xi $的样本组成的集合，则条件概率 $P(xi∣c) $可估计为：
   $$
   P\left(x_i \mid c\right)=\frac{\left|D_{c, x_i}\right|}{\left|D_c\right|}
   $$
   对于连续属性可考虑概率密度函数，假定$p\left(x_i \mid c\right) \sim \mathcal{N}\left(\mu_{c, i}, \sigma_{c, i}^2\right)$ ，其中 $μ_c,i$ 和 $σ_{c,i}^2$ 分别是第$ c $类样本在第 i 个属性上取值的均值和方差，则有：
   $$
   P\left(x_i \mid c\right)=\frac{1}{\sqrt{2 \pi} \sigma_{c, i}} \exp \left(-\frac{\left(x_i-\mu_{c, i}\right)^2}{2 \sigma_{c, i}^2}\right)
   $$

4. 算法流程

   ![img](C:\Program Files\Typora\v2-c6c960202ec62afaf53b2764f726228b_720w.webp)

   在直接使用极大似然估计法时，需要注意，若某个属性值在训练集中没有与某个类同时出现，则直接基于之前的公式进行概率估计，再进行判别将出现问题。例如，当我们判断一个人是否感冒，给出的属性包含：$年龄={少年，中年，老年}$；$是否头痛={是，否}$，如果当前我们的训练集中没有包含少年人群的数据，此时，如果来了一个新的数据是少年且头痛，那么：

   $P_{\text {少年|感冒 }}=P($ 年龄 $=$ 少年 $\mid$ 是否感冒 $=$ 是 $)=0 ，$

   上式等于 0 的原因就是我们的训练数据集中没有  **年龄 = 少年**  的数据，然而，经验告诉我们，少年且头痛也是很有可能感冒的，这显然不太合理。

### HMM(Hidden Markov Model)

